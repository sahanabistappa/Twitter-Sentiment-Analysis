{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6bKEAlWNsXYH",
        "pAsQku_99_79"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "AIM: To perform sentiment analysis and prediction of sentiment using Tweets about US Airlines.\n"
      ],
      "metadata": {
        "id": "6bKEAlWNsXYH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QzSySETLKmo"
      },
      "outputs": [],
      "source": [
        "# importing the required libraries & packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import argparse\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from datetime import datetime\n",
        "import pytz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Import: Kaggle\n",
        "1.   loading the US Airline Sentiment data\n",
        "2.   Procedure link: https://www.analyticsvidhya.com/blog/2021/06/how-to-load-kaggle-datasets-directly-into-google-colab/\n",
        "\n"
      ],
      "metadata": {
        "id": "Bx52P2Z99QPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhBg59urz-Ri",
        "outputId": "73fcaab1-edc6-476c-af55-a4841d4bdaee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2022.5.18.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (6.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.64.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "WIfYoIDu8giL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94b7de2-4616-4f63-a2af-e2032003a2ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "DMlclMAM8gnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download crowdflower/twitter-airline-sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19KB6fd98pCy",
        "outputId": "fe4bc38b-e85f-45fe-b996-37020611b4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "twitter-airline-sentiment.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip twitter-airline-sentiment.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4FCYSJK91bx",
        "outputId": "5a19694e-e724-47e2-e22d-94e89450a5eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  twitter-airline-sentiment.zip\n",
            "replace Tweets.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace database.sqlite? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the data\n",
        "We've imported the dataset from Kaggle and now we're going\n",
        "to read the data using pandas/ Let's see some descriptive stats!"
      ],
      "metadata": {
        "id": "pAsQku_99_79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset as a DataFrame\n",
        "df = pd.read_csv(\"Tweets.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "2kjPfoMO-ONF",
        "outputId": "152b2ae7-c0b2-44a8-a590-9cf67db79be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "0  570306133677760513           neutral                        1.0000   \n",
              "1  570301130888122368          positive                        0.3486   \n",
              "2  570301083672813571           neutral                        0.6837   \n",
              "3  570301031407624196          negative                        1.0000   \n",
              "4  570300817074462722          negative                        1.0000   \n",
              "\n",
              "  negativereason  negativereason_confidence         airline  \\\n",
              "0            NaN                        NaN  Virgin America   \n",
              "1            NaN                     0.0000  Virgin America   \n",
              "2            NaN                        NaN  Virgin America   \n",
              "3     Bad Flight                     0.7033  Virgin America   \n",
              "4     Can't Tell                     1.0000  Virgin America   \n",
              "\n",
              "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
              "0                    NaN     cairdin                 NaN              0   \n",
              "1                    NaN    jnardino                 NaN              0   \n",
              "2                    NaN  yvonnalynn                 NaN              0   \n",
              "3                    NaN    jnardino                 NaN              0   \n",
              "4                    NaN    jnardino                 NaN              0   \n",
              "\n",
              "                                                text tweet_coord  \\\n",
              "0                @VirginAmerica What @dhepburn said.         NaN   \n",
              "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
              "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
              "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
              "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
              "\n",
              "               tweet_created tweet_location               user_timezone  \n",
              "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
              "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
              "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
              "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
              "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-57153b02-8f58-4343-a737-de0515bde057\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-57153b02-8f58-4343-a737-de0515bde057')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-57153b02-8f58-4343-a737-de0515bde057 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-57153b02-8f58-4343-a737-de0515bde057');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPD930-7-7-f",
        "outputId": "10fc4ee8-2321-40ff-de49-019d7f166cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14640, 15)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Info: We have about 15 columns in the dataset. We're using the following columns:\n",
        "1. 'text': used as X, the feature\n",
        "2.  'airline_segment': as Y, the target label for the classifier.\n",
        "\n",
        "The values in the target data are categorical and are 'neutral', 'positive' & 'negative'. For the easy computation, we are replacing the 'neutral', 'positive' & 'negative' with 0, 1 & 2 values respectively.\n"
      ],
      "metadata": {
        "id": "vMWpoacE_InL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing the categorical values of 'airline_sentiment' to numeric values\n",
        "df['airline_sentiment'].replace(('neutral', 'positive', 'negative'), (0, 1, 2), inplace=True)\n",
        "df['airline_sentiment'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7g98wxWBb4y",
        "outputId": "3ccbeec0-4d52-44c1-ad42-48b7432ce8f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    9178\n",
              "0    3099\n",
              "1    2363\n",
              "Name: airline_sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#forming the feature & label variables\n",
        "data = df['text'].values.tolist()\n",
        "labels = df['airline_sentiment'].values.tolist()"
      ],
      "metadata": {
        "id": "UifxLz6MBjUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first five samples text\n",
        "data[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psfTta5YB0PU",
        "outputId": "abc415b9-9529-43ed-aa18-e081fb3af4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@VirginAmerica What @dhepburn said.',\n",
              " \"@VirginAmerica plus you've added commercials to the experience... tacky.\",\n",
              " \"@VirginAmerica I didn't today... Must mean I need to take another trip!\",\n",
              " '@VirginAmerica it\\'s really aggressive to blast obnoxious \"entertainment\" in your guests\\' faces &amp; they have little recourse',\n",
              " \"@VirginAmerica and it's a really big bad thing about it\"]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# first 5 samples label\n",
        "labels[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebYHbcAfB5Av",
        "outputId": "7810654d-1803-4fea-c1ce-a5f2c95bed96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 0, 2, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the data for classification\n",
        "We use the conventional 80-20 split using 'train_test_split' from sklearn"
      ],
      "metadata": {
        "id": "zVbh135tCJxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data into 80 and 20 split\n",
        "train_X, test_X, y_train, y_test = train_test_split(data, labels, test_size=0.2,\n",
        "                                                    random_state=42, shuffle=True)\n",
        "\n",
        "print(f'Number of training examples: {len(train_X)}')\n",
        "print(f'Number of testing examples: {len(test_X)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFkZa9g1DDhL",
        "outputId": "4eff9f91-12ef-45ea-e294-5b340e073fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 11712\n",
            "Number of testing examples: 2928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "freq = collections.Counter(y_train)\n",
        "print(dict(freq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCAhM4BibG8I",
        "outputId": "0190753e-f9e2-4a80-a432-6a3285567805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{2: 7289, 1: 1904, 0: 2519}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing\n",
        "A process of transforming text into something an algorithm can digest is text processing. This includes:\n",
        "\n",
        "1. Tokenizing the data\n",
        "https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/\n",
        "Character tokens: s-m-a-r-t-e-r\n",
        "Subword tokens: smart-er\n",
        "\n",
        "\n",
        "2. Removing the punctuation\n",
        "3. Removing the stopwords\n",
        "4. Stemming\n",
        "5. Lemmatization\n",
        "As of now, we are only going to tokenize the data and work with it without removing the punctuation or stop words and apply any other text processing methods.\n"
      ],
      "metadata": {
        "id": "JESqC_IqDPrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adding a default pattern for tokenization\n",
        "default_pattern =  r\"\"\"(?x)\n",
        "                        (?:[A-Z]\\.)+\n",
        "                        |\\$?\\d+(?:\\.\\d+)?%?\n",
        "                        |\\w+(?:[-']\\w+)*\n",
        "                        |\\.\\.\\.\n",
        "                        |(?:[.,;\"'?():-_`])\n",
        "                    \"\"\""
      ],
      "metadata": {
        "id": "H4AOuWu6Dlfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function for tokenizing the data\n",
        "\"\"\" Tokenize sentence with specific pattern\n",
        "Arguments: text {str} -- sentence to be tokenized, such as \"I love NLP\"\n",
        "Keyword Arguments: pattern {str} -- reg-expression pattern for tokenizer (default: {default_pattern})\n",
        "Returns: list -- list of tokenized words, such as ['I', 'love', 'nlp'] \"\"\"\n",
        "def tokenize(text, pattern = default_pattern):\n",
        "\n",
        "  text = text.lower()\n",
        "  return regexp_tokenize(text, pattern)"
      ],
      "metadata": {
        "id": "lm-dO0iyFd3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building the Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "aMa-veP5GIXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Classifier class is defined as NBClasssifier; with 4 functions:\n",
        "\n",
        "\n",
        "*   createDictionary()\n",
        "\n",
        "*   fit()\n",
        "\n",
        "\n",
        "*   predict()\n",
        "\n",
        "*   score()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NZdGC69PGpgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**createDictionary():** This function takes in the tokenized text data and gives out the dictionary or the bag of words of the data.\n",
        "\n",
        "**fit():** This function has all the word counts required to calculate the Navie Bayes Classifier probabilities and then fits the classifier on our training data.\n",
        "\n",
        "**predict():** The test data is inputed to this function which determines the sentiment label based of each tweet by using the word counts computed during the training process (from fit function). In this step, Laplace smoothing is applied while computing Naïve Bayes probabilities for the test data.\n",
        "\n",
        "score(): Determine how many tweets are classified correctly and measures the performance of the model in terms of accuracy."
      ],
      "metadata": {
        "id": "-9vsn7CgHFHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Navie Bayes Classifier\n",
        "class NBClassifier:\n",
        "\n",
        "    def __init__(self, X_train, y_train, size):\n",
        "      tz_NY = pytz.timezone('America/New_York')\n",
        "      print(\"Model Start Time:\", datetime.now(tz_NY).strftime(\"%H:%M:%S\"))\n",
        "      self.X_train = X_train\n",
        "      self.y_train = y_train\n",
        "      self.size = size\n",
        "\n",
        "    def createDictionary(self):\n",
        "      \"\"\" Function: To create a dictionary of tokens from the data\n",
        "      Arguments: data [features] in the type - list\n",
        "      Returns: Sorted dictionary of the tokens and their count in the data \"\"\"\n",
        "      # x-train has tweet data\n",
        "      dictionary = dict()\n",
        "      # iterating through training tweet data\n",
        "      for sample in  X_train:\n",
        "        for token in sample:\n",
        "          dictionary[token] = dictionary.get(token, 0) + 1\n",
        "      #sorting the dictionary based on the values\n",
        "      sorted_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
        "      return dict(sorted_dict)\n",
        "\n",
        "    def fit(self):\n",
        "      \"\"\" Function: To compute the count of words in training data dictionary\n",
        "        Arguments: Training data & Size of dictionary\n",
        "        Returns: dictionary of tokens with their class wise probabilities \"\"\"\n",
        "\n",
        "      X_train_dict = self.createDictionary()\n",
        "      if self.size == 'full':\n",
        "        self.words_list = list(X_train_dict.keys())\n",
        "        self.words_count = dict.fromkeys(self.words_list, None)\n",
        "      else:\n",
        "        self.words_list = list(X_train_dict.keys())[:int(self.size)]\n",
        "        self.words_count = dict.fromkeys(self.words_list, None)\n",
        "\n",
        "      #DataFrame of training data\n",
        "      train = pd.DataFrame(columns = ['X_train', 'y_train'])\n",
        "      train['X_train'] = X_train\n",
        "      train['y_train'] = y_train\n",
        "\n",
        "      train_0 = train.copy()[train['y_train'] == 0]\n",
        "      train_1 = train.copy()[train['y_train'] == 1]\n",
        "      train_2 = train.copy()[train['y_train'] == 2]\n",
        "\n",
        "      #computing the prior of each class\n",
        "      Pr0 = train_0.shape[0]/train.shape[0]\n",
        "      Pr1 = train_1.shape[0]/train.shape[0]\n",
        "      Pr2 = train_2.shape[0]/train.shape[0]\n",
        "\n",
        "      self.Prior = np.array([Pr0, Pr1, Pr2])\n",
        "\n",
        "      #converting list of lists into a list\n",
        "      def flatList(listOfList):\n",
        "        flatten = []\n",
        "        for elem in listOfList:\n",
        "          flatten.extend(elem)\n",
        "        return flatten\n",
        "\n",
        "      #Creating the data list for each class - tokens of each class\n",
        "      X_train_0 = flatList(train[train['y_train'] == 0]['X_train'].tolist())\n",
        "      X_train_1 = flatList(train[train['y_train'] == 1]['X_train'].tolist())\n",
        "      X_train_2 = flatList(train[train['y_train'] == 2]['X_train'].tolist())\n",
        "\n",
        "      self.X_train_len = np.array([len(X_train_0), len(X_train_1), len(X_train_2)])\n",
        "\n",
        "      for token in self.words_list:\n",
        "        #list to store three word counts of a token\n",
        "        res = []\n",
        "\n",
        "        #inserting count of token in class 0: Neutral\n",
        "        res.insert(0, X_train_0.count(token))\n",
        "\n",
        "        #inserting count of token in class 1: Positive\n",
        "        res.insert(1, X_train_1.count(token))\n",
        "\n",
        "          #inserting count of token in class 2: Negative\n",
        "        res.insert(2, X_train_2.count(token))\n",
        "\n",
        "        #assigning the count list to its token in the dictionary\n",
        "        self.words_count[token] = res\n",
        "      return self\n",
        "\n",
        "    def predict(self, X_test):\n",
        "      \"\"\" Function: Predicts the label of the data\n",
        "        Arguments: self and the test data\n",
        "        Returns: List of predicted labels for the test data \"\"\"\n",
        "      pred = []\n",
        "      for sample in X_test:\n",
        "        mul = np.array([1,1,1])\n",
        "        for tokens in sample:\n",
        "          vocab_count = len(self.words_list)\n",
        "          if tokens in self.words_list:\n",
        "            prob = ((np.array(self.words_count[tokens])+1) / (self.X_train_len + vocab_count))\n",
        "          #except:\n",
        "            #prob = ((np.array([0,0,0])+1) / (self.X_train_len + vocab_count))\n",
        "          mul = mul * prob\n",
        "        val = mul * self.Prior\n",
        "        pred.append(np.argmax(val))\n",
        "      tz_NY = pytz.timezone('America/New_York')\n",
        "      print(\"Model End Time:\", datetime.now(tz_NY).strftime(\"%H:%M:%S\"))\n",
        "      return pred\n",
        "\n",
        "    def score(self, pred, labels):\n",
        "      \"\"\" Function: To compute the perfoemance of the model\n",
        "        Arguments: self, predicted labels and actual labels of the test data\n",
        "        Returns: Number of lables correctly predicted and the accuracy of the model \"\"\"\n",
        "      correct = (np.array(pred) == np.array(labels)).sum()\n",
        "      accuracy = correct/len(pred)\n",
        "      return correct, accuracy"
      ],
      "metadata": {
        "id": "vUiLxJ75Gkjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Navie Bayes Classifier Training and Evaluation\n",
        "The Navie Bayes Classifier, NBClassifier takes three arguments:\n",
        "\n",
        "X_train: Features of training dataset\n",
        "y_train: Labels of training dataset\n",
        "size: Size of vacabulary to be used in the model\n",
        "All three arguments are needed for the model to work."
      ],
      "metadata": {
        "id": "LigzqH5vIJ0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating holders to store the model performance results\n",
        "attributes = []\n",
        "corr = []\n",
        "acc = []\n",
        "\n",
        "#function to call for storing the results\n",
        "def storeResults(attr, cor,ac):\n",
        "  attributes.append(attr)\n",
        "  corr.append(round(cor, 3))\n",
        "  acc.append(round(ac, 3))"
      ],
      "metadata": {
        "id": "3beTAU43IOSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#training the classifier\n",
        "nb = NBClassifier(X_train, y_train, 'full')\n",
        "nb.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred))"
      ],
      "metadata": {
        "id": "xMowLBM4IRXB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92815816-9e74-4814-aedd-c8f6cde0fcee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Start Time: 04:54:35\n",
            "Model End Time: 04:56:06\n",
            "NBClassifier Model miss any prediction??? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor1, acc1 = nb.score(y_pred, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor1)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor1, len(y_pred), acc1))"
      ],
      "metadata": {
        "id": "-jCmCm09IUJz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0beccce5-9f45-4328-bf52-68a015832e2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of Correct Predictions: 2291\n",
            "Accuracy of the model: 2291 / 2928 = 0.7824 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('Unprocessed Data', cor1, acc1)"
      ],
      "metadata": {
        "id": "Gz_ZyJqBIX9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Navie Bayers Classifier that we trainined on the data predicts 78.24% of samples correctly. Now, to improve this number few more text processing methods are appilied on the training data and then the classifier is trained on this modified data to predict the sentiment of the test samples."
      ],
      "metadata": {
        "id": "awHSbfOpIaCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Trying to improve the NBClassifier\n",
        "To improve the performance of the NBClassifier,\n",
        "\n",
        "apply other text processing methods\n",
        "reduce the size of dictonary\n",
        "\n",
        "\n",
        "**9.1. Further Processing Text Data**\n",
        "\n",
        "In this step, we are going to apply two text processing methods on the previously tokenized data:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   remove punctuation\n",
        "*   remove stop words\n",
        "\n",
        "\n",
        "**9.1.1. Remove Puntuation**"
      ],
      "metadata": {
        "id": "LMEIICviIdYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#string of punctiations\n",
        "string.punctuation\n"
      ],
      "metadata": {
        "id": "jYRwFbS2I6SB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "a39939a5-c888-4991-89f0-8e5653a24b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the punctuation\n",
        "'''Function: Removes the punctuation from the tokens\n",
        "   Arguments: list of text data samples\n",
        "   Returns: list of tokens of each sample without punctuation '''\n",
        "def removePunctuation(data):\n",
        "    update = []\n",
        "    for sample in data:\n",
        "        #removing punctuation from the tokens\n",
        "        re_punct = [''.join(char for char in word if char not in string.punctuation) for word in sample]\n",
        "        #removes the empty strings\n",
        "        re_punct = [word for word in re_punct if word]\n",
        "\n",
        "        update.append(re_punct)\n",
        "    return update\n"
      ],
      "metadata": {
        "id": "yGQ_m7IZI83p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Removing punctuation from training data text tokens\n",
        "X_train_P = removePunctuation(X_train)\n",
        "\n",
        "#Removing punctuation from testing data text tokens\n",
        "X_test_P = removePunctuation(X_test)\n",
        "\n",
        "#train & test data after removing punctuation\n",
        "print(X_train_P[0])\n",
        "print(X_test_P[0])"
      ],
      "metadata": {
        "id": "IN7SRV7TI_L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc750508-7356-4215-d413-b1726bba55cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['united', 'you', 'are', 'offering', 'us', '8', 'rooms', 'for', '32', 'people', 'fail']\n",
            "['southwestair', 'youre', 'my', 'early', 'frontrunner', 'for', 'best', 'airline', 'oscars2016']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training the classifier\n",
        "nb_punct = NBClassifier(X_train_P, y_train, 'full')\n",
        "nb_punct.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_P = nb_punct.predict(X_test_P)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_P))"
      ],
      "metadata": {
        "id": "Bhkyrv58JBTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3132cf15-a2b0-45b7-a7e6-3497c7224cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Start Time: 04:56:07\n",
            "Model End Time: 04:57:28\n",
            "NBClassifier Model miss any prediction??? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor2, acc2 = nb_punct.score(y_pred_P, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor2)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor2, len(y_pred_P), acc2))"
      ],
      "metadata": {
        "id": "VQprb0miJDuN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a03fea4-773f-478d-cc38-8d9b2302121c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of Correct Predictions: 2285\n",
            "Accuracy of the model: 2285 / 2928 = 0.7804 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('No Punctuation Data', cor2, acc2)"
      ],
      "metadata": {
        "id": "3v8NPyUIJFtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.1.2 Remove Stop words**"
      ],
      "metadata": {
        "id": "YFXq_-AxJJtX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Function: Removes the stopwords from the tokens\n",
        "   Arguments: list of text data samples\n",
        "   Returns: list of tokens of each sample without punctuation '''\n",
        "def removeStopWords(data):\n",
        "    update = []\n",
        "    stopwords = ['the', 'at','i', 'of', 'us', 'have', 'a', 'you','ours', 'themselves',\n",
        "                 'that', 'this', 'be', 'is', 'for']\n",
        "    for sample in data:\n",
        "        #removing stopwords from tokenized data\n",
        "        re_stop = [word for word in sample if word not in stopwords]\n",
        "\n",
        "        update.append(re_stop)\n",
        "    return update"
      ],
      "metadata": {
        "id": "X7sMXdtlJO4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing stopwords from training data text tokens\n",
        "X_train_S = removeStopWords(X_train)\n",
        "\n",
        "#Removing stopwords from testing data text tokens\n",
        "X_test_S = removeStopWords(X_test)\n",
        "\n",
        "#train & test data after removing stopwords\n",
        "print(X_train_S[0])\n",
        "print(X_test_S[0])"
      ],
      "metadata": {
        "id": "7bz0SFbXJRHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9794eaf-141d-4dc6-9f2b-041f9c5dd4e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@', 'united', 'are', 'offering', '8', 'rooms', '32', 'people', 'fail']\n",
            "['@', 'southwestair', \"you're\", 'my', 'early', 'frontrunner', 'best', 'airline', 'oscars2016']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training the classifier\n",
        "nb_stop = NBClassifier(X_train_S, y_train, 'full')\n",
        "nb_stop.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_S = nb_stop.predict(X_test_S)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_S))"
      ],
      "metadata": {
        "id": "g6oeRAOQJTW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05059c6b-52ae-4213-c593-23a565a97624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Start Time: 04:57:29\n",
            "Model End Time: 04:58:48\n",
            "NBClassifier Model miss any prediction??? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor3, acc3 = nb_stop.score(y_pred_S, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor3)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor3, len(y_pred_S), acc3))"
      ],
      "metadata": {
        "id": "tmL-Kl6DJTgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfa0ad4-91ea-4edd-cab3-df70cf62a279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of Correct Predictions: 2300\n",
            "Accuracy of the model: 2300 / 2928 = 0.7855 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('Removed few Stopwords', cor3, acc3)"
      ],
      "metadata": {
        "id": "nZEevWljJiAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.1.3. Removing both Punctuation & Few Stopwords**"
      ],
      "metadata": {
        "id": "T_1xKSdwJjB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing stopwords from training data text tokens\n",
        "X_train_PS = removeStopWords(X_train_P)\n",
        "\n",
        "#Removing stopwords from testing data text tokens\n",
        "X_test_PS = removeStopWords(X_test_P)\n",
        "\n",
        "#train & test data after removing stopwords\n",
        "print(X_train_PS[0])\n",
        "print(X_test_PS[0])"
      ],
      "metadata": {
        "id": "ZWsvcLtZJlwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f74bedd-1ddc-4109-b656-9d83f6032d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['united', 'are', 'offering', '8', 'rooms', '32', 'people', 'fail']\n",
            "['southwestair', 'youre', 'my', 'early', 'frontrunner', 'best', 'airline', 'oscars2016']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training the classifier\n",
        "nb_PS = NBClassifier(X_train_PS, y_train, 'full')\n",
        "nb_PS.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_PS = nb_PS.predict(X_test_PS)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_PS))"
      ],
      "metadata": {
        "id": "8h747l1xJn2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7d11a04-f93e-49a0-d471-936ac67993c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Start Time: 04:58:49\n",
            "Model End Time: 05:00:08\n",
            "NBClassifier Model miss any prediction??? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor4, acc4 = nb_PS.score(y_pred_PS, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor4)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor4, len(y_pred_PS), acc4))"
      ],
      "metadata": {
        "id": "dsMMBfUqLab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47ac244-16e3-4746-ca77-fd36d9929244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of Correct Predictions: 2283\n",
            "Accuracy of the model: 2283 / 2928 = 0.7797 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('Removed both Punctuation & Few Stopwords', cor4, acc4)"
      ],
      "metadata": {
        "id": "WyEocoSsLbFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.2. Reducing the Dictionary Size**\n",
        "\n",
        "To improve the model performance, we reduce the size of training dictionary further by taking only top-k frequent word types that appear in it. Here, we vary the value of k and compare the model performance."
      ],
      "metadata": {
        "id": "p9EtGLdMLekp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#total tokens in training dictionary\n",
        "print('Total tokens in the dictionary:', len(bog))"
      ],
      "metadata": {
        "id": "c4GMmXfNLkCT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62211885-1f67-48f2-aab1-9a5d6e22b944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens in the dictionary: 13606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSMMLFISLvQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.2.1. Considering Top 5k Tokens**\n",
        "\n",
        "**5k Tokens of Vocabulary - Unprocessed data**"
      ],
      "metadata": {
        "id": "mLBhSv3_L--m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the classifier - 5000 tokens\n",
        "nb_5k = NBClassifier(X_train, y_train, '5000')\n",
        "nb_5k.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_5k = nb_5k.predict(X_test)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k))"
      ],
      "metadata": {
        "id": "CLoaq5jIMFKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e355af5-ad4b-40e0-a93b-9430c255a222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Start Time: 05:00:09\n",
            "Model End Time: 05:00:39\n",
            "NBClassifier Model miss any prediction??? False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor5, acc5 = nb_5k.score(y_pred_5k, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor5)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor5, len(y_pred), acc5))"
      ],
      "metadata": {
        "id": "Jte4BgXAMQEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "764bdf90-6abe-4899-eb08-50d8d690b18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of Correct Predictions: 2332\n",
            "Accuracy of the model: 2332 / 2928 = 0.7964 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('5k Tokens of Voab - Unprocessed Data', cor5, acc5)"
      ],
      "metadata": {
        "id": "VSlboqC5MRyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5k Tokens of Vocabulary - No Punctuation Data**"
      ],
      "metadata": {
        "id": "fSoVf3Z3MTfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the classifier - 5000 tokens\n",
        "nb_5k_P = NBClassifier(X_train_P, y_train, '5000')\n",
        "nb_5k_P.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_5k_P = nb_5k_P.predict(X_test_P)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k_P))"
      ],
      "metadata": {
        "id": "IZVI_UiTMUnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d7a215a-4178-4b51-a63a-9d8280ab0a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Start Time: 05:00:40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor6, acc6 = nb_5k.score(y_pred_5k_P, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor6)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor6, len(y_pred), acc6))"
      ],
      "metadata": {
        "id": "-_-rVLWNMZVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('5k Tokens of Voab - No Punctuation Data', cor6, acc6)"
      ],
      "metadata": {
        "id": "DGm1t3lhMZx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5k Tokens of Vocabulary - Removed few Stopwords**"
      ],
      "metadata": {
        "id": "rxCswPZiMbe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the classifier - 5000 tokens\n",
        "nb_5k_S = NBClassifier(X_train_S, y_train, '5000')\n",
        "nb_5k_S.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_5k_S = nb_5k_S.predict(X_test_S)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k_S))"
      ],
      "metadata": {
        "id": "9h_YkgLIMdvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Performance of the classifier\n",
        "cor7, acc7 = nb_5k_S.score(y_pred_5k_S, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor7)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor7, len(y_pred), acc7))"
      ],
      "metadata": {
        "id": "-OnY8FiQMgbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('5k Tokens of Voab - Removed few Stopwords', cor7, acc7)"
      ],
      "metadata": {
        "id": "65Tf7sr4Mjoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5k Tokens of Vocabulary - Removed both Punctuation & Few Stopwords**"
      ],
      "metadata": {
        "id": "0r5MZfXHMm1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the classifier - 5000 tokens\n",
        "nb_5k_PS = NBClassifier(X_train_PS, y_train, '5000')\n",
        "nb_5k_PS.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_5k_PS = nb_5k_PS.predict(X_test_PS)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_5k_PS))"
      ],
      "metadata": {
        "id": "dTkaZDacMn8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor8, acc8 = nb_5k_PS.score(y_pred_5k_PS, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor8)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor8, len(y_pred), acc8))"
      ],
      "metadata": {
        "id": "vmdsqzfYMqA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('5k Tokens of Voab - Removed both Punctuation & Few Stopwords', cor8, acc8)"
      ],
      "metadata": {
        "id": "LTYj0zxuMs-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9.2.2. Considering Top 10k Tokens**\n",
        "\n",
        "**10k Tokens of Vocabulary - Unprocessed data**"
      ],
      "metadata": {
        "id": "avwpXqh6MvaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the classifier - 5000 tokens\n",
        "nb_10k = NBClassifier(X_train, y_train, '5000')\n",
        "nb_10k.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_10k = nb_10k.predict(X_test)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_10k))"
      ],
      "metadata": {
        "id": "3JYtT_ueM3zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor9, acc9 = nb_10k.score(y_pred_10k, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor9)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor9, len(y_pred), acc9))"
      ],
      "metadata": {
        "id": "2BodKr0YM89O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('10k Tokens of Voab - Unprocessed Data', cor9, acc9)"
      ],
      "metadata": {
        "id": "nO_4maX6M-oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10k Tokens of Vocabulary - No Punctuation Data**"
      ],
      "metadata": {
        "id": "efQ6ZtECNBbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the classifier - 10000 tokens\n",
        "nb_10k_P = NBClassifier(X_train_P, y_train, '10000')\n",
        "nb_10k_P.fit()\n",
        "\n",
        "#predicting the labels for test samples\n",
        "y_pred_10k_P = nb_10k_P.predict(X_test_P)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any prediction???\", len(X_test) != len(y_pred_10k_P))"
      ],
      "metadata": {
        "id": "9VtK-QSoNCvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor10, acc10 = nb_10k_P.score(y_pred_10k_P, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor10)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor10, len(y_pred), acc10))"
      ],
      "metadata": {
        "id": "BvpuFCFVNE21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('10k Tokens of Voab - No Punctuation Data', cor10, acc10)"
      ],
      "metadata": {
        "id": "oNQmYhUlNI1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10k Tokens of Vocabulary - Removed few Stopwords**"
      ],
      "metadata": {
        "id": "2459fsHYNKec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the classifier - 10000 tokens\n",
        "nb_10k_S = NBClassifier(X_train_S, y_train, '10000')\n",
        "nb_10k_S.fit()\n",
        "\n",
        "#Sredicting the labels for test samSles\n",
        "y_pred_10k_S = nb_10k_S.predict(X_test_S)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClassifier Model miss any Srediction???\", len(X_test) != len(y_pred_10k_S))"
      ],
      "metadata": {
        "id": "8wER58vkNL2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor11, acc11 = nb_10k_S.score(y_pred_10k_S, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor11)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor11, len(y_pred), acc11))"
      ],
      "metadata": {
        "id": "FFc7bTHmNTw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('10k Tokens of Voab - Removed few Stopwords', cor11, acc11)"
      ],
      "metadata": {
        "id": "BlYMrwwfNWrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10k Tokens of Vocabulary - Removed both Punctuation & Few Stopword**"
      ],
      "metadata": {
        "id": "iH6zSSO6NZPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training the claPSPSifier - 10000 tokenPS\n",
        "nb_10k_PS = NBClassifier(X_train_PS, y_train, '10000')\n",
        "nb_10k_PS.fit()\n",
        "\n",
        "#PSredicting the labelPS for tePSt PSamPSlePS\n",
        "y_pred_10k_PS = nb_10k_PS.predict(X_test_PS)\n",
        "\n",
        "#Checking\n",
        "print(\"NBClaPSPSifier Model miSS any PSrediction???\", len(X_test) != len(y_pred_10k_PS))"
      ],
      "metadata": {
        "id": "zVtTd4DhNaxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance of the classifier\n",
        "cor12, acc12 = nb_10k_PS.score(y_pred_10k_PS, y_test)\n",
        "print(\"Count of Correct Predictions:\", cor12)\n",
        "print(\"Accuracy of the model: %i / %i = %.4f \" %(cor12, len(y_pred), acc12))"
      ],
      "metadata": {
        "id": "oajqrDVLNdM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#storing the results. The below mentioned order of parameter passing is important.\n",
        "#Caution: Execute only once to avoid duplications.\n",
        "storeResults('10k Tokens of Voab - Removed both Punctuation & Few Stopwords', cor12, acc12)"
      ],
      "metadata": {
        "id": "AueRLZ5ENgYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Comparing the Results"
      ],
      "metadata": {
        "id": "qMJFvOT9Niea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dataframe\n",
        "results = pd.DataFrame({ 'Data Modification': attributes,\n",
        "    'Correct Predictions': corr,\n",
        "    'Model Accuracy': acc})"
      ],
      "metadata": {
        "id": "N6fjpYXnNnpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.sort_values(by=['Model Accuracy', 'Correct Predictions'], ascending=False)"
      ],
      "metadata": {
        "id": "DGSkSILjNrtH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "d0a4c463-4024-48c5-c567-a3f6a0747637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    Data Modification  Correct Predictions  \\\n",
              "4                5k Tokens of Voab - Unprocessed Data                 2332   \n",
              "8               10k Tokens of Voab - Unprocessed Data                 2332   \n",
              "6           5k Tokens of Voab - Removed few Stopwords                 2321   \n",
              "10         10k Tokens of Voab - Removed few Stopwords                 2321   \n",
              "5             5k Tokens of Voab - No Punctuation Data                 2309   \n",
              "2                               Removed few Stopwords                 2300   \n",
              "7   5k Tokens of Voab - Removed both Punctuation &...                 2296   \n",
              "11  10k Tokens of Voab - Removed both Punctuation ...                 2293   \n",
              "0                                    Unprocessed Data                 2291   \n",
              "9            10k Tokens of Voab - No Punctuation Data                 2287   \n",
              "1                                 No Punctuation Data                 2285   \n",
              "3            Removed both Punctuation & Few Stopwords                 2283   \n",
              "\n",
              "    Model Accuracy  \n",
              "4            0.796  \n",
              "8            0.796  \n",
              "6            0.793  \n",
              "10           0.793  \n",
              "5            0.789  \n",
              "2            0.786  \n",
              "7            0.784  \n",
              "11           0.783  \n",
              "0            0.782  \n",
              "9            0.781  \n",
              "1            0.780  \n",
              "3            0.780  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc9fd397-543f-4b53-97da-3c3db16d4a4f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data Modification</th>\n",
              "      <th>Correct Predictions</th>\n",
              "      <th>Model Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5k Tokens of Voab - Unprocessed Data</td>\n",
              "      <td>2332</td>\n",
              "      <td>0.796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>10k Tokens of Voab - Unprocessed Data</td>\n",
              "      <td>2332</td>\n",
              "      <td>0.796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5k Tokens of Voab - Removed few Stopwords</td>\n",
              "      <td>2321</td>\n",
              "      <td>0.793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10k Tokens of Voab - Removed few Stopwords</td>\n",
              "      <td>2321</td>\n",
              "      <td>0.793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5k Tokens of Voab - No Punctuation Data</td>\n",
              "      <td>2309</td>\n",
              "      <td>0.789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Removed few Stopwords</td>\n",
              "      <td>2300</td>\n",
              "      <td>0.786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5k Tokens of Voab - Removed both Punctuation &amp;...</td>\n",
              "      <td>2296</td>\n",
              "      <td>0.784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>10k Tokens of Voab - Removed both Punctuation ...</td>\n",
              "      <td>2293</td>\n",
              "      <td>0.783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Unprocessed Data</td>\n",
              "      <td>2291</td>\n",
              "      <td>0.782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10k Tokens of Voab - No Punctuation Data</td>\n",
              "      <td>2287</td>\n",
              "      <td>0.781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>No Punctuation Data</td>\n",
              "      <td>2285</td>\n",
              "      <td>0.780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Removed both Punctuation &amp; Few Stopwords</td>\n",
              "      <td>2283</td>\n",
              "      <td>0.780</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc9fd397-543f-4b53-97da-3c3db16d4a4f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bc9fd397-543f-4b53-97da-3c3db16d4a4f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bc9fd397-543f-4b53-97da-3c3db16d4a4f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KPKzfVwgzSZW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}